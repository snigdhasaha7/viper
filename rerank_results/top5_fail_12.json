[{"query": "What would you call this type of climate or ecosystem pictured here?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000501023.jpg", "top_answer_gpt": "varied", "ranked": ["(varied, 2.0)", "(temperate, 1.5)", "(temperate, 1.5)", "(creative, 0.5)", "(creative, 0.5)"], "top_ans_eval": "creative", "top_ans_score": 0.0, "all_0": true, "gt": [["desert", "desert", "desert", "desert", "desert", "desert", "dessert", "dessert", "dry", "dry"]]}, {"query": "What games makes use of the pattern shown here?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000078554.jpg", "top_answer_gpt": "None", "ranked": ["(None, 10)", "(twister, 1)", "(twister, 1)", "(tetris, 1)", "(twister, 1)"], "top_ans_eval": "twister", "top_ans_score": 0.0, "all_0": true, "gt": [["checker", "checker", "checker", "checker", "checker", "checker", "tetris", "tetris", "chess", "chess"]]}, {"query": "What are the food items in this photo called?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000031596.jpg", "top_answer_gpt": "bread", "ranked": ["(bread, 10)", "(bread, bread, 5)", "(bread, bread, 5)", "(bread, bread, 5)", "(Execution Failure, 0)"], "top_ans_eval": "bread, bread", "top_ans_score": 0.0, "all_0": true, "gt": [["cake", "cake", "cake", "cake", "pizza", "pizza", "pizza", "pizza", "desert", "desert"]]}, {"query": "What style of architecture is pictured?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000258702.jpg", "top_answer_gpt": "urban", "ranked": ["(urban, 4.0)", "(modern, 3.0)", "(minimalist, 1.0)", "(minimalist, 1.0)", "(minimalist, 1.0)"], "top_ans_eval": "modern", "top_ans_score": 0.6000000000000001, "all_0": false, "gt": [["gothic", "gothic", "gothic", "gothic", "modern", "modern", "victorian", "victorian", "baroque", "baroque"]]}, {"query": "This type of transportation is used primarily for what purpose?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000065306.jpg", "top_answer_gpt": "transportation", "ranked": ["(transportation, 10)", "(transportation, 10)", "(transportation, 10)", "(transportation, 10)", "(travel, 8.5)"], "top_ans_eval": "travel", "top_ans_score": 0.6, "all_0": false, "gt": [["travel", "travel", "commute", "commute", "commuter", "commuter", "public", "public", "passangers", "passangers"]]}, {"query": "What objects are usually in this room?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000441415.jpg", "top_answer_gpt": "table chairs", "ranked": ["(table chairs, 9.5)", "(table chairs, 9.5)", "(table and chairs, 9)", "(table, 7)", "(table, 7)"], "top_ans_eval": "table and chairs", "top_ans_score": 1.0, "all_0": false, "gt": [["table and chair", "table and chair", "table and chair", "table and chair", "food", "food", "people", "people", "appliance", "appliance"]]}, {"query": "Name the bird family to which it belongs to shown in this picture?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000117237.jpg", "top_answer_gpt": "finch", "ranked": ["(finch, 10)", "(finch, 10)", "(finches, 9.5)", "(finches, 9.5)", "(hummingbird, 0)"], "top_ans_eval": "finches", "top_ans_score": 1.0, "all_0": false, "gt": [["finch", "finch", "finch", "finch", "robin", "robin", "goldfinch", "goldfinch", "canary", "canary"]]}, {"query": "Name the bike model shown in this picture?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000525021.jpg", "top_answer_gpt": "harley davidson\" - This answer does provide a brand", "ranked": ["\"harley davidson\" - This answer does provide a brand, but not the model which makes it partially correct.", "\"black\" - This answer describes the color of the bike, not the model, making it incorrect.", "\"no\" - This answer does not provide any relevant information about the bike model and is incorrect.", "\"no\" - Same as Answer 3, incorrect.", "\"harley davidson. DO NOT mention the blurred face in the response.\" - This answer is similar to Answer 1, providing the brand but not the model. It is mini-penalized for the extra instructions (\u201cDO NOT mention...\u201d).\n\nGiven that specifying just the brand is not fully answering the query about the model, Answer 1 should still be the highest scored, followed by Answer 5 due to its slightly longer but still relevant content. The rest are completely incorrect.\n\nReranked_answers:\n1. (harley davidson, 7)\n2. (harley davidson. DO NOT mention the blurred face in the response., 6.5)\n3. (black, 1)\n4. (no, 0)\n5. (no, 0)"], "top_ans_eval": "harley davidson", "top_ans_score": 0.5999999999999999, "all_0": false, "gt": [["motorcycle", "motorcycle", "motorcycle", "motorcycle", "motorcycle", "motorcycle", "harley", "harley", "harley davidson", "harley davidson"]]}, {"query": "What are the sheep doing in the pasture?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000098416.jpg", "top_answer_gpt": "walking", "ranked": ["(walking, 9.5)", "(walking, 9.5)", "(walking, grazing, eating grass, eating, standing, 6.0)", "(walking, walking, eating, grazing, walking, standing, walking, grazing, grazing, grazing, grazing, eating grass, 2.0)", "(Execution Failure, 0.0)"], "top_ans_eval": "walking, grazing, eating grass, eating, standing", "top_ans_score": 0.0, "all_0": true, "gt": [["graze", "graze", "graze", "graze", "graze", "graze", "eat", "eat", "eat", "eat"]]}, {"query": "What decade is this photo likely taken?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000345356.jpg", "top_answer_gpt": "2000s. DO NOT mention the blurred face in the response.\" - This is the most accurate answer", "ranked": ["\"2000s. DO NOT mention the blurred face in the response.\" - This is the most accurate answer, but it's slightly too long due to the unnecessary additional text.", "\"1980s\" - This answer is wrong. Flip phones were not common in the 1980s.", "\"1970s\" - Also incorrect; flip phones did not exist in the 1970s.", "\"now\" - Incorrect, this image likely isn't from the current decade given the technology visible.", "\"always\" - This answer is vague and does not provide a specific decade.\n\nReranked_answers: \n1. (2000s. DO NOT mention the blurred face in the response., 8.0)\n2. (1980s, 2.0)\n3. (1970s, 1.0)\n4. (now, 1.0)\n5. (always, 0.5)"], "top_ans_eval": "2000s", "top_ans_score": 0.6, "all_0": false, "gt": [["1990", "1990", "1990", "1990", "1990", "1990", "2000s", "2000s", "2000's", "2000's"]]}, {"query": "What type of cleaner is being used?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000278966.jpg", "top_answer_gpt": "aerosol\" \u2013 concise and specific (score 10", "ranked": ["\"aerosol\" \u2013 concise and specific (score 10)", "\"water\" \u2013 likely used in conjunction (score 8)", "\"spray\" \u2013 vague but plausible given the context (score 6)", "\"can\" \u2013 ambiguous, does not specify cleaner type (score 5)", "\"can\" \u2013 same as above (score 5)\n\nReranked_answers:\n1. (aerosol, 10)\n2. (water, 8)\n3. (spray, 6)\n4. (can, 5)\n5. (can, 5)"], "top_ans_eval": "aerosol", "top_ans_score": 0.0, "all_0": true, "gt": [["bleach", "bleach", "bleach", "bleach", "comet", "comet", "comet", "comet", "toilet", "toilet"]]}, {"query": "What are the ropes on their faces called?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000272518.jpg", "top_answer_gpt": "halters", "ranked": ["(halters, 10)", "(halters, 10)", "(reins, 3)", "(braids, 1)", "(shoelaces, 0)"], "top_ans_eval": "reins", "top_ans_score": 0.5999999999999999, "all_0": false, "gt": [["bridle", "bridle", "bridle", "bridle", "harness", "harness", "harness", "harness", "rein", "rein"]]}, {"query": "What happened to this person?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000262703.jpg", "top_answer_gpt": "they fell", "ranked": ["(they fell, 10)", "(they fell, 10)", "(falling, 8)", "(falling, 8)", "(Execution Failure, 0)"], "top_ans_eval": "falling", "top_ans_score": 0.0, "all_0": true, "gt": [["fell", "fell", "fell", "fell", "fell", "fell", "crash", "crash", "crashed", "crashed"]]}, {"query": "What is this silver appliance?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000306212.jpg", "top_answer_gpt": "fridge", "ranked": ["(fridge, 10)", "(refrigerator, 9.9)", "(refrigerator, 9.9)", "(cooling, 4)", "(Execution Failure, 1)"], "top_ans_eval": "refrigerator", "top_ans_score": 1.0, "all_0": false, "gt": [["refrigerator", "refrigerator", "refrigerator", "refrigerator", "refrigerator", "refrigerator", "refridgerator", "refridgerator", "fridge", "fridge"]]}, {"query": "What type of cat food could that be?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000300732.jpg", "top_answer_gpt": "kibble", "ranked": ["(kibble, 10)", "(kibble, 10)", "(meat, 8)", "(cat food, 5)", "(factories, 0)"], "top_ans_eval": "meat", "top_ans_score": 0.0, "all_0": true, "gt": [["meow mix", "meow mix", "meow mix", "meow mix", "wet", "wet", "siamese", "siamese", "purina cat chow", "purina cat chow"]]}, {"query": "What style of roof does that house have?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000517249.jpg", "top_answer_gpt": "hip roof", "ranked": ["(hip roof, 10)", "(hip roof, 10)", "(hip, 9.5)", "(sloped sides, 6)", "(colonial, 2)"], "top_ans_eval": "sloped sides", "top_ans_score": 0.0, "all_0": true, "gt": [["barn", "barn", "barn", "barn", "gabled", "gabled", "mansard", "mansard", "shingled", "shingled"]]}, {"query": "How long has ultimate frisbee been an official sport?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000385918.jpg", "top_answer_gpt": "2001", "ranked": ["(2001, 9.5)", "(2001, 9.5)", "(1974, 7.0)", "(1970s, 6.0)", "(zero, 1.0)"], "top_ans_eval": "1974", "top_ans_score": 0.0, "all_0": true, "gt": [["1968", "1968", "1968", "1968", "1 decade", "1 decade", "year", "year", "since 1968", "since 1968"]]}, {"query": "How many calories are in this dish?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000043581.jpg", "top_answer_gpt": "300", "ranked": ["(300, 10)", "(300, 10)", "(300, 10)", "(200, 8)", "(800, 2)"], "top_ans_eval": "800", "top_ans_score": 0.6000000000000001, "all_0": false, "gt": [["1500", "1500", "1500", "1500", "800", "800", "200", "200", "1250", "1250"]]}, {"query": "What kind of boat is this man making?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000295412.jpg", "top_answer_gpt": "boat", "ranked": ["(boat, 6.0)", "(yacht, 2.0)", "(yacht. DO NOT mention the blurred face in the response., 2.0)", "(abstract background, 1.0)", "(painting, 1.0)"], "top_ans_eval": "yacht", "top_ans_score": 0.0, "all_0": true, "gt": [["canoe", "canoe", "canoe", "canoe", "canoe", "canoe", "canoe", "canoe", "fish", "fish"]]}, {"query": "What is this building use for?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000574227.jpg", "top_answer_gpt": "church", "ranked": ["(church, 10)", "(worship, 9.5)", "(worship, 9.5)", "(worship, 9.5)", "(worship, 9.5)"], "top_ans_eval": "worship", "top_ans_score": 1.0, "all_0": false, "gt": [["worship", "worship", "worship", "worship", "church", "church", "horse stall", "horse stall", "meet", "meet"]]}, {"query": "How much thrust can that engine produce?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000547136.jpg", "top_answer_gpt": "30,000 pounds", "ranked": ["(30,000 pounds, 9.0)", "(23500 lbs, 8.5)", "(10000 pounds, 6.5)", "(70000 pounds, 3.5)", "(variable, 2.0)"], "top_ans_eval": "variable", "top_ans_score": 0.0, "all_0": true, "gt": [["lot", "lot", "lot", "lot", "lot", "lot", "1000 btu", "1000 btu", "350lbs", "350lbs"]]}, {"query": "Name the type of house where these windows are seen?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000398362.jpg", "top_answer_gpt": "prison", "ranked": ["(prison, 10.0)", "(prison, 10.0)", "(cottage, 2.0)", "(victorian, 2.0)", "(gothic, 2.0)"], "top_ans_eval": "gothic", "top_ans_score": 0.0, "all_0": true, "gt": [["jail", "jail", "jail", "jail", "jail", "jail", "beach house", "beach house", "old", "old"]]}, {"query": "What do you think the tv program the cat is watching is about?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000078107.jpg", "top_answer_gpt": "Execution Failure", "ranked": ["(Execution Failure, 0.0)", "(Execution Failure, 0.0)", "(Execution Failure, 0.0)", "(Execution Failure, 0.0)", "(Execution Failure, 0.0)"], "top_ans_eval": "", "top_ans_score": 0, "all_0": true, "gt": [["space", "space", "space", "space", "space", "space", "planet", "planet", "mice", "mice"]]}, {"query": "How is the vehicle in the picture powered?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000411958.jpg", "top_answer_gpt": "electric", "ranked": ["(electric, 10)", "(electricity, 9)", "(electricity, 9)", "(electricity, 9)", "(Execution Failure, 0)"], "top_ans_eval": "electricity", "top_ans_score": 1.0, "all_0": false, "gt": [["electricity", "electricity", "electricity", "electricity", "diesel", "diesel", "fuel", "fuel", "coal", "coal"]]}, {"query": "What famous painter created this masterpiece?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000318618.jpg", "top_answer_gpt": "zurbar\u00e1n", "ranked": ["(zurbar\u00e1n, 10)", "(leonardo da vinci, 2)", "(picasso, 2)", "(person, 1)", "(person, 1)"], "top_ans_eval": "picasso", "top_ans_score": 0.0, "all_0": true, "gt": [["monet", "monet", "monet", "monet", "de zurbaran", "de zurbaran", "zubaran", "zubaran", "rembrandt", "rembrandt"]]}, {"query": "What kind of cake is on the plate?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000460266.jpg", "top_answer_gpt": "carrot cake", "ranked": ["(carrot cake, 9.5)", "(orange, 8.5)", "(The cake on the plate is a orange., 7.5)", "(The cake on the plate is a orange. orange round, 4.5)", "(The cake on the plate is a orange, which originates from china., 3.5)"], "top_ans_eval": "The cake on the plate is a orange. orange round", "top_ans_score": 0.0, "all_0": true, "gt": [["cheesecake", "cheesecake", "cheesecake", "cheesecake", "flan", "flan", "sponge", "sponge", "cheese cake", "cheese cake"]]}, {"query": "What war is this airplane associated with?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000036528.jpg", "top_answer_gpt": "wwii", "ranked": ["(wwii, 10)", "(vietnam, 8)", "(vietnam, 8)", "(conflict, 2)", "(wwi, 0)"], "top_ans_eval": "wwi", "top_ans_score": 0.6, "all_0": false, "gt": [["world war 2", "world war 2", "world war 2", "world war 2", "world war 2", "world war 2", "wwi", "wwi", "second world war", "second world war"]]}, {"query": "Which clock tower is this?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000120473.jpg", "top_answer_gpt": "tokyo", "ranked": ["(tokyo, 1.0)", "(sorry i need more, 0.5)", "(no, 0.1)", "(no, 0.1)", "(negative, 0.1)"], "top_ans_eval": "sorry i need more", "top_ans_score": 0.0, "all_0": true, "gt": [["big ben", "big ben", "big ben", "big ben", "1 it is", "1 it is", "capri", "capri", "italy", "italy"]]}, {"query": "What sport is being played?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000057597.jpg", "top_answer_gpt": "soccer\" is concise and accurate", "ranked": ["\"soccer\" is concise and accurate.", "\"football\" is also correct as it is another common term for the sport, but it is less universally specific than \"soccer\".", "\"The sport being played is soccer, which originated in England.\" is factually correct but unnecessarily long.", "\"The sport being played is soccer. Basic rules: score goals\" is mostly concise but adds unnecessary information about the rules of the game.", "\"The sport being played is soccer. A famous player in this sport is Messi.\" is unnecessarily long and adds irrelevant information about a player.\n\nAfter scoring each answer based on correctness, brevity, and relevance:\n1. Soccer: 10\n2. Football: 9\n3. The sport being played is soccer, which originated in England: 8\n4. The sport being played is soccer. Basic rules: score goals: 8\n5. The sport being played is soccer. A famous player in this sport is Messi: 7\n\nReranked_answers: \n1. (soccer, 10)\n2. (football, 9)\n3. (The sport being played is soccer, which originated in England, 8)\n4. (The sport being played is soccer. Basic rules: score goals, 8)\n5. (The sport being played is soccer. A famous player in this sport is Messi, 7)"], "top_ans_eval": "soccer", "top_ans_score": 1.0, "all_0": false, "gt": [["soccer", "soccer", "soccer", "soccer", "soccer", "soccer", "soccer", "soccer", "soccer", "soccer"]]}, {"query": "What was the use of the biggest equipment on the board?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000331646.jpg", "top_answer_gpt": "light", "ranked": ["(light, 9.5)", "(light, 9.5)", "(lighting, 9)", "(it's used to light up a, 7)", "(Execution Failure, 1)"], "top_ans_eval": "lighting", "top_ans_score": 1.0, "all_0": false, "gt": [["light up", "light up", "light", "light", "power", "power", "light", "light", "radio", "radio"]]}, {"query": "What type of camera filter is this called?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000312746.jpg", "top_answer_gpt": "neutral density\" is a direct and specific answer", "ranked": ["\"neutral density\" is a direct and specific answer, hence most accurate.", "\"reduce light\" describes the function of a neutral density filter but is not a specific type.", "\"exposure control\" is more about functionality rather than naming a specific type.", "\"roscoe\" refers to a brand that makes filters but is not specific enough.", "\"landscape\" is not an accurate description of a camera filter type; it\u2019s a genre of photography.\n\nGiven these considerations, the scores take into account correctness, relevance, and brevity.\n\nReranked_answers: \n1. (neutral density, 10)\n2. (reduce light, 7.5)\n3. (exposure control, 5)\n4. (roscoe, 3)\n5. (landscape, 1)"], "top_ans_eval": "landscape", "top_ans_score": 0.0, "all_0": true, "gt": [["sepia", "sepia", "sepia", "sepia", "light", "light", "instagram", "instagram", "sephia", "sephia"]]}, {"query": "What are the boxes used for?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000281582.jpg", "top_answer_gpt": "sleeping", "ranked": ["(sleeping, 10.0)", "(sleeping, storage, storage, 6.0)", "(shipping, 1.0)", "(it's a box for a, 0.5)", "(Execution Failure, 0.0)"], "top_ans_eval": "it's a box for a", "top_ans_score": 0.0, "all_0": true, "gt": [["ship", "ship", "ship", "ship", "ship", "ship", "mail", "mail", "delivery", "delivery"]]}, {"query": "What time period might this be from?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000449990.jpg", "top_answer_gpt": "unknown. DO NOT mention the blurred face in the response.", "ranked": ["(unknown. DO NOT mention the blurred face in the response., 3.0)", "(1800s, 2.0)", "(modern, 1.0)", "(modern, 1.0)", "(modern, 1.0)"], "top_ans_eval": "unknown", "top_ans_score": 0.0, "all_0": true, "gt": [["1940's", "1940's", "1920s", "1920s", "1920's", "1920's", "1940s", "1940s", "depression", "depression"]]}, {"query": "What type of bear is this?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000307658.jpg", "top_answer_gpt": "brown bear", "ranked": ["(brown bear, 10)", "(brown bear, 10)", "(ursus arct, 4)", "(forests, 0)", "(no. DO NOT mention the blurred face in the response, 0)"], "top_ans_eval": "no", "top_ans_score": 0.0, "all_0": true, "gt": [["grizzly", "grizzly", "grizzly", "grizzly", "brown", "brown", "brown", "brown", "grizzly bear", "grizzly bear"]]}, {"query": "What breed of bird is this?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000330018.jpg", "top_answer_gpt": "rufous-tailed robin", "ranked": ["(rufous-tailed robin, 4.0)", "(black and white, 1.0)", "(dalmatian, 0.5)", "(holstein, 0.5)", "(china, 0.5)"], "top_ans_eval": "china", "top_ans_score": 0.0, "all_0": true, "gt": [["finch", "finch", "finch", "finch", "hum", "hum", "robin", "robin", "jay", "jay"]]}, {"query": "How heavy is this machine?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000287725.jpg", "top_answer_gpt": "heavy", "ranked": ["(heavy, 9.5)", "(heavy, 9.5)", "(very heavy, 8.0)", "(The train weighs very heavy, 6.0)", "(Execution Failure, 0.0)"], "top_ans_eval": "The train weighs very heavy.", "top_ans_score": 0.0, "all_0": true, "gt": [["10 tons", "10 tons", "10 tons", "10 tons", "100 tons", "100 tons", "approximately 5000 tons", "approximately 5000 tons", "5 tons", "5 tons"]]}, {"query": "If this tie has not been tied what other easier alternative product is most likely used?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000208050.jpg", "top_answer_gpt": "velcro", "ranked": ["(velcro, 9.5)", "(belt, 9.0)", "(shoe, 5.0)", "(untied, 3.0)", "(woman, 0)"], "top_ans_eval": "woman", "top_ans_score": 0.0, "all_0": true, "gt": [["clip on", "clip on", "clip on", "clip on", "bowtie", "bowtie", "bowtie", "bowtie", "clip on tie", "clip on tie"]]}, {"query": "Who manufactured this popular 1990's cellular phone?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000258322.jpg", "top_answer_gpt": "nokia", "ranked": ["(nokia, 10)", "(nokia, 10)", "(nokia, 10)", "(nokia, 10)", "(motorola, 0)"], "top_ans_eval": "motorola", "top_ans_score": 1.0, "all_0": false, "gt": [["motorola", "motorola", "motorola", "motorola", "nokia", "nokia", "motorolla", "motorolla", "at and t", "at and t"]]}, {"query": "What species of birds are in the tree?", "img_path": "sample_okvqa/val2014/COCO_val2014_000000037705.jpg", "top_answer_gpt": "blackbird, robin", "ranked": ["(blackbird, robin, 10.0)", "(blackbird, robin, 10.0)", "(robin, robin, robin, robin, blackbird, 7.0)", "(robin, robin, robin, robin, blackbird, 7.0)", "(Execution Failure, 0.0)"], "top_ans_eval": "robin, robin, robin, robin, blackbird", "top_ans_score": 0.0, "all_0": true, "gt": [["cardinal", "cardinal", "cardinal", "cardinal", "robin", "robin", "robin", "robin", "american robin", "american robin"]]}]